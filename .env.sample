# Agentic RAG — Environment Configuration Sample
# Copy to .env and adjust values as needed

# ----------------------------
# Ollama / LLM configuration
# ----------------------------
# Base URL for your local or remote Ollama server
# Default in code: http://127.0.0.1:11434
OLLAMA_API_BASE=http://127.0.0.1:11434

# Primary chat/completion model (Litellm routing). Example: ollama/llama3.2:1b
LLM_MODEL_NAME=ollama/llama3.2:1b
# Async streaming model id (raw Ollama id). Example: llama3.2:1b
ASYNC_LLM_MODEL_NAME=llama3.2:1b
# Sampling temperature for LLM responses (0.0 – 1.0)
LLM_TEMPERATURE=0.1
# Embedding model for FAISS index
EMBED_MODEL_NAME=sentence-transformers/paraphrase-MiniLM-L3-v2

# ----------------------------
# MCP server (FastMCP streamable HTTP)
# ----------------------------
# Host/port/path for the MCP server entrypoint
MCP_HOST=127.0.0.1
MCP_PORT=8000
# Path segment for the MCP streamable HTTP transport
MCP_PATH=/mcp
# Soft process memory limit in MB (monitor + GC; containerize for hard caps)
MAX_MEMORY_MB=2048

# ----------------------------
# REST server (FastAPI)
# ----------------------------
# Host/port for the REST API server
RAG_HOST=127.0.0.1
RAG_PORT=8001
# Base path prefix for REST routes (e.g., /api)
RAG_PATH=api
# Optional: override memory limit for REST only (falls back to MAX_MEMORY_MB)
REST_MAX_MEMORY_MB=2048

# ----------------------------
# Data paths and behavior
# ----------------------------
# JSONL store path (relative or absolute)
RAG_DB=./cache/rag_store.jsonl
# Optional working directory base to resolve relative inputs
# RAG_WORKDIR=/absolute/path/to/workdir
# Enable lightweight debug mode (skips heavy model loads)
RAG_DEBUG_MODE=false
