# ==============================================
# Agentic RAG Configuration
# ==============================================

# ----------------------------------------------
# 1. Ollama Configuration
# ----------------------------------------------
# URL for the Ollama server (default: http://127.0.0.1:11434)
# If set to a remote URL, local Ollama startup is skipped.
OLLAMA_API_BASE=http://127.0.0.1:11434

# Duration models stay loaded in memory (-1 = indefinite, 5m = 5 minutes)
OLLAMA_KEEP_ALIVE=-1

# Context window size (default: 1024 for standard, 4096 for high-RAM arm64)
# OLLAMA_NUM_CTX=1024

# Models to use
LLM_MODEL_NAME=ollama/llama3.2:1b
ASYNC_LLM_MODEL_NAME=llama3.2:1b

# ----------------------------------------------
# 2. Server Configuration
# ----------------------------------------------
# MCP Server (Model Context Protocol)
MCP_HOST=127.0.0.1
MCP_PORT=8000
# HTTP path for the MCP transport
MCP_PATH=/mcp

# REST API Server (RAG Backend)
RAG_HOST=127.0.0.1
RAG_PORT=8001
# Base path for API routes (default: api)
RAG_PATH=api

# UI Development Server
UI_HOST=0.0.0.0
UI_PORT=5173

# Memory limits for REST server (in MB)
# MAX_MEMORY_MB=4096

# ----------------------------------------------
# 3. Application Mode
# ----------------------------------------------
# Operation mode: 'local' (monolith) or 'remote' (client connecting to server)
RAG_MODE=local

# URL for remote RAG server (only used if RAG_MODE=remote)
RAG_REMOTE_URL=http://127.0.0.1:8001/api

# Enable debug mode (true/false)
RAG_DEBUG_MODE=false

# Path to the persisted document store
RAG_DB=./cache/rag_store.jsonl

# ----------------------------------------------
# 4. Paths & Caching
# ----------------------------------------------
# Custom paths for model caches (optional)
# SENTENCE_TRANSFORMERS_HOME=/path/to/cache
# TORCH_HOME=/path/to/torch/cache
# HF_HOME=/path/to/huggingface/cache

# Offline mode for Hugging Face Hub (1 = enabled)
# HF_HUB_OFFLINE=1

# ----------------------------------------------
# 5. Performance & Threading (Intel/MKL)
# ----------------------------------------------
# Threading controls for FAISS/NumPy stability on Intel CPUs
# These are automatically set by start.sh but can be overridden here
# OMP_NUM_THREADS=1
# MKL_NUM_THREADS=1
# OPENBLAS_NUM_THREADS=1
# VECLIB_MAXIMUM_THREADS=1
# NUMEXPR_NUM_THREADS=1

# Torch installation pinning (optional)
# TORCH_VERSION=2.2.2
# TORCH_INDEX_URL=https://download.pytorch.org/whl/cpu

# ----------------------------------------------
# 6. External APIs (Optional)
# ----------------------------------------------
# OpenAI API Base URL (if using OpenAI-compatible endpoints)
# OPENAI_API_BASE=https://api.openai.com/v1

# Hugging Face Embedding Model
EMBED_MODEL_NAME=sentence-transformers/paraphrase-MiniLM-L3-v2

# ----------------------------------------------
# 7. Google Backend Configuration (Optional)
# ----------------------------------------------
# Mode: 'manual' (Drive API + Gemini) or 'vertex_ai_search' (Vertex AI Grounding)
GOOGLE_GROUNDING_MODE=manual

# Model to use for Google Backend (e.g., models/gemini-2.0-flash, models/gemini-1.5-pro)
# You can use 'models/gemini-3.0-pro' if you have access.
GOOGLE_MODEL_NAME=models/gemini-2.0-flash

# Vertex AI Configuration (Required if GOOGLE_GROUNDING_MODE=vertex_ai_search)
# VERTEX_PROJECT_ID=your-project-id
# VERTEX_LOCATION=global
# VERTEX_DATA_STORE_ID=your-data-store-id
