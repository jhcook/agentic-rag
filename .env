# Copy this file to `.env` and adjust values to override defaults.

#EMBED_MODEL_NAME=sentence-transformers/paraphrase-MiniLM-L3-v2  # Hugging Face identifier for the embedding model
EMBED_MODEL_NAME=Snowflake/arctic-embed-xs
#LLM_MODEL_NAME=ollama/llama3.2:1b  # LiteLLM/Ollama completion model name
LLM_MODEL_NAME=ollama/qwen2.5:3b
#ASYNC_LLM_MODEL_NAME=llama3.2:1b  # Model name for AsyncClient-based chat calls (no provider prefix)
ASYNC_LLM_MODEL_NAME=qwen2.5:3b
OLLAMA_API_BASE=http://127.0.0.1:11434  # Base URL where `ollama serve` is listening
OLLAMA_KEEP_ALIVE=-1  # Keep models loaded indefinitely (-1) or for specified duration (e.g., 24h, 30m)

RAG_PATH=api # Base path prefix for REST routes
RAG_DB=./cache/rag_store.jsonl  # Path to the persisted document store
RAG_DEBUG_MODE=false  # Set to 'true' to skip expensive embedding operations for testing/development
REST_MAX_MEMORY_MB=50  # Soft memory ceiling for REST server; falls back to MAX_MEMORY_MB if unset

#MAX_MEMORY_MB=32768  # Soft memory ceiling for http_server; 75% of system memory if unset
MCP_PATH=/mcp  # HTTP path for the MCP transport
MCP_HOST=127.0.0.1  # Host/IP for http_server
MCP_PORT=8000  # Port for http_server
