# Copy this file to `.env` and adjust values to override defaults.

#EMBED_MODEL_NAME=sentence-transformers/paraphrase-MiniLM-L3-v2  # Hugging Face identifier for the embedding model
EMBED_MODEL_NAME=Snowflake/arctic-embed-xs
#LLM_MODEL_NAME=ollama/llama3.2:1b  # LiteLLM/Ollama completion model name
LLM_MODEL_NAME=ollama/qwen2.5:0.5b
#ASYNC_LLM_MODEL_NAME=llama3.2:1b  # Model name for AsyncClient-based chat calls (no provider prefix)
ASYNC_LLM_MODEL_NAME=qwen2.5:0.5b
OLLAMA_API_BASE=http://127.0.0.1:11434  # Base URL where `ollama serve` is listening
RAG_DB=./cache/rag_store.jsonl  # Path to the persisted document store

MAX_MEMORY_MB=4096  # Soft memory ceiling for http_server; 75% of system memory if unset
MCP_PATH=/mcp  # HTTP path for the MCP transport
MCP_HOST=127.0.0.1  # Host/IP for http_server
MCP_PORT=8000  # Port for http_server
